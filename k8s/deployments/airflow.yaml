# Airflow Webserver Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-webserver
  namespace: data-platform
  labels:
    app: airflow-webserver
    app.kubernetes.io/part-of: data-platform
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow-webserver
  template:
    metadata:
      labels:
        app: airflow-webserver
        app.kubernetes.io/part-of: data-platform
    spec:
      serviceAccountName: airflow-account
      securityContext:
        fsGroup: 1000
        runAsNonRoot: true
      containers:
        - name: airflow-webserver
          image: airflow-spark:1.0.0  # Use specific version tags
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
              name: web
          command: ["/bin/bash"]
          args:
            - -c
            - |
              airflow db init &&
              airflow users create -r Admin -u admin -p $AIRFLOW_ADMIN_PASSWORD -e admin@example.com -f Admin -l User &&
              airflow webserver --ssl-cert /opt/airflow/certs/airflow.crt --ssl-key /opt/airflow/certs/airflow.key
          env:
            - name: AIRFLOW__CORE__EXECUTOR
              value: LocalExecutor
            - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
              valueFrom:
                secretKeyRef:
                  name: airflow-secrets
                  key: db-connection
            - name: AIRFLOW_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-secrets
                  key: admin-password
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-secrets
                  key: db-password
            - name: AIRFLOW__CORE__LOAD_EXAMPLES
              value: "false"
            - name: AIRFLOW_CONN_SPARK_DEFAULT
              value: spark://spark-master:7077
            - name: SPARK_HOME
              value: /opt/spark
            # Security configurations
            - name: AIRFLOW__WEBSERVER__RBAC
              value: "True"
            - name: AIRFLOW__CORE__SECURE_MODE
              value: "True"
            - name: AIRFLOW__WEBSERVER__WEB_SERVER_SSL_CERT
              value: "/opt/airflow/certs/airflow.crt"
            - name: AIRFLOW__WEBSERVER__WEB_SERVER_SSL_KEY  
              value: "/opt/airflow/certs/airflow.key"
            - name: AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX
              value: "True"
          securityContext:
            allowPrivilegeEscalation: false
            runAsUser: 1000
            runAsGroup: 1000
            readOnlyRootFilesystem: false  # Airflow may need to write to filesystem
            capabilities:
              drop:
                - ALL
          resources:
            requests:
              memory: "1Gi"
              cpu: "0.5"
            limits:
              memory: "2Gi"
              cpu: "1"
          livenessProbe:  
            httpGet:
              path: /health
              port: web
              scheme: HTTPS
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: web
              scheme: HTTPS
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          volumeMounts:
            - name: dags
              mountPath: /opt/airflow/dags
              readOnly: true
            - name: spark-tests
              mountPath: /opt/spark/tests
              readOnly: true
            - name: airflow-certs
              mountPath: /opt/airflow/certs
              readOnly: true
      volumes:
        - name: dags
          configMap:
            name: airflow-dags
        - name: spark-tests
          configMap:
            name: spark-tests
        - name: airflow-certs
          secret:
            secretName: airflow-ssl-certs
---
# Airflow Scheduler Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-scheduler
  namespace: data-platform
  labels:
    app: airflow-scheduler
    app.kubernetes.io/part-of: data-platform
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow-scheduler
  template:
    metadata:
      labels:
        app: airflow-scheduler
        app.kubernetes.io/part-of: data-platform
    spec:
      serviceAccountName: airflow-account
      securityContext:
        fsGroup: 1000
        runAsNonRoot: true
      containers:
        - name: airflow-scheduler
          image: airflow-spark:1.0.0
          imagePullPolicy: IfNotPresent
          command: ["/bin/bash"]
          args:
            - -c
            - |
              airflow db upgrade &&
              airflow scheduler
          env:
            - name: AIRFLOW__CORE__EXECUTOR
              value: LocalExecutor
            - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
              valueFrom:
                secretKeyRef:
                  name: airflow-secrets
                  key: db-connection
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-secrets
                  key: db-password
            - name: AIRFLOW__CORE__LOAD_EXAMPLES
              value: "false"
            - name: AIRFLOW_CONN_SPARK_DEFAULT
              value: spark://spark-master:7077
            - name: AIRFLOW__CORE__SECURE_MODE
              value: "True"
          securityContext:
            allowPrivilegeEscalation: false
            runAsUser: 1000
            runAsGroup: 1000
            readOnlyRootFilesystem: false  # Scheduler may need to write temp files
            capabilities:
              drop:
                - ALL
          resources:
            requests:
              memory: "1Gi"
              cpu: "0.5"
            limits:
              memory: "2Gi"
              cpu: "1"
          volumeMounts:
            - name: dags
              mountPath: /opt/airflow/dags
              readOnly: true
            - name: spark-tests
              mountPath: /opt/spark/tests
              readOnly: true
      volumes:
        - name: dags
          configMap:
            name: airflow-dags
        - name: spark-tests
          configMap:
            name: spark-tests
