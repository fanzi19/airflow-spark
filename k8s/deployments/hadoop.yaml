# NameNode Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: namenode
  namespace: data-platform
  labels:
    app: namenode
    app.kubernetes.io/component: hadoop
    app.kubernetes.io/part-of: data-platform
spec:
  replicas: 1
  selector:
    matchLabels:
      app: namenode
  template:
    metadata:
      labels:
        app: namenode
        app.kubernetes.io/component: hadoop
        app.kubernetes.io/part-of: data-platform
    spec:
      serviceAccountName: hadoop-sa
      # Pod security context - use non-root user
      securityContext:
        fsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
      initContainers:
      - name: config-init
        image: airflow-spark:1.0.0
        imagePullPolicy: IfNotPresent
        # Container security context
        securityContext:
          allowPrivilegeEscalation: false
          runAsUser: 1000
          runAsGroup: 1000
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: false
        command:
        - sh
        - -c
        - |
          set -ex
          echo "Copying configuration files..."
          # Create target directory
          mkdir -p /config-rw
          
          # Copy files
          cp -r /config-mount/* /config-rw/
          
          # Set correct permissions for Hadoop to run as non-root
          chmod -R 755 /config-rw/
        resources:
          limits:
            cpu: "200m"
            memory: "256Mi"
          requests:
            cpu: "100m"
            memory: "128Mi"
        volumeMounts:
        - mountPath: /config-mount
          name: hadoop-config-ro
          readOnly: true
        - mountPath: /config-rw
          name: hadoop-config-rw
      containers:
        - name: namenode
          image: airflow-spark:1.0.0
          imagePullPolicy: IfNotPresent
          # Container security context
          securityContext:
            allowPrivilegeEscalation: false
            runAsUser: 1000
            runAsGroup: 1000
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          command: ["/bin/bash"]
          args:
          - -c
          - |
            set -ex
            echo "Starting namenode with secure configuration..."
            # Modify bootstrap.sh to run as non-root (assuming your bootstrap.sh supports it)
            /bootstrap.sh namenode
          ports:
            - containerPort: 9870
              name: http
            - containerPort: 9000
              name: rpc
          env:
            - name: HADOOP_CONF_DIR
              value: /opt/hadoop/etc/hadoop
            - name: SERVICE_TYPE
              value: hadoop
            - name: HADOOP_HOME
              value: /opt/hadoop
            # Use non-root users for Hadoop services
            - name: HDFS_NAMENODE_USER
              value: hdfs
            - name: HDFS_DATANODE_USER
              value: hdfs
            - name: HDFS_SECONDARYNAMENODE_USER
              value: hdfs
            - name: YARN_RESOURCEMANAGER_USER
              value: yarn
            - name: YARN_NODEMANAGER_USER
              value: yarn
            # Security settings
            - name: HADOOP_OPTS
              value: "-Djava.security.properties=/opt/hadoop/etc/hadoop/java.security"
            - name: HADOOP_SECURE_LOG_DIR
              value: /opt/hadoop/logs
          envFrom:
            - configMapRef:
                name: hadoop-env
          resources:
            limits:
              cpu: "2"
              memory: "4Gi"
            requests:
              cpu: "1"
              memory: "2Gi"
          volumeMounts:
            - name: hadoop-config-rw
              mountPath: /opt/hadoop/etc/hadoop
            - name: hadoop-namenode-storage
              mountPath: /opt/hadoop/dfs/name
            - name: hadoop-logs
              mountPath: /opt/hadoop/logs
          livenessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 90
            periodSeconds: 30
            timeoutSeconds: 15
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 60
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 3
            successThreshold: 1
      volumes:
        - name: hadoop-namenode-storage
          persistentVolumeClaim:
            claimName: hadoop-namenode
        - name: hadoop-logs
          persistentVolumeClaim:
            claimName: hadoop-logs
        - name: hadoop-config-ro
          configMap:
            name: hadoop-config
            defaultMode: 0440 # Read-only for owner and group
        - name: hadoop-config-rw
          emptyDir:
            medium: Memory # Use memory for better security
            sizeLimit: 20Mi
---
# DataNode Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: datanode
  namespace: data-platform
  labels:
    app: datanode
    app.kubernetes.io/component: hadoop
    app.kubernetes.io/part-of: data-platform
spec:
  replicas: 1
  selector:
    matchLabels:
      app: datanode
  template:
    metadata:
      labels:
        app: datanode
        app.kubernetes.io/component: hadoop
        app.kubernetes.io/part-of: data-platform
    spec:
      serviceAccountName: hadoop-sa
      # Pod security context
      securityContext:
        fsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
      initContainers:
      - name: config-init
        image: airflow-spark:1.0.0
        imagePullPolicy: IfNotPresent
        securityContext:
          allowPrivilegeEscalation: false
          runAsUser: 1000
          runAsGroup: 1000
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: false
        command:
        - sh
        - -c
        - |
          set -ex
          echo "Copying configuration files..."
          mkdir -p /config-rw
          cp -r /config-mount/* /config-rw/
          chmod -R 755 /config-rw/
        resources:
          limits:
            cpu: "200m"
            memory: "256Mi"
          requests:
            cpu: "100m"
            memory: "128Mi"
        volumeMounts:
        - mountPath: /config-mount
          name: hadoop-config-ro
          readOnly: true
        - mountPath: /config-rw
          name: hadoop-config-rw
      containers:
        - name: datanode
          image: airflow-spark:1.0.0
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            runAsUser: 1000
            runAsGroup: 1000
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          command: ["/bin/bash"]
          args: ["-c", "/bootstrap.sh datanode"]
          ports:
            - containerPort: 9864
              name: http
            - containerPort: 9866
              name: data
          env:
            - name: SERVICE_TYPE
              value: hadoop
            - name: HADOOP_HOME
              value: /opt/hadoop
            - name: HDFS_DATANODE_USER
              value: hdfs
            - name: HADOOP_CONF_DIR
              value: /opt/hadoop/etc/hadoop
            - name: HADOOP_OPTS
              value: "-Djava.security.properties=/opt/hadoop/etc/hadoop/java.security"
          envFrom:
            - configMapRef:
                name: hadoop-env
          resources:
            limits:
              cpu: "2"
              memory: "4Gi"
            requests:
              cpu: "1"
              memory: "2Gi"
          volumeMounts:
            - name: hadoop-config-rw
              mountPath: /opt/hadoop/etc/hadoop
            - name: hadoop-datanode-storage
              mountPath: /opt/hadoop/dfs/data
          livenessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 90
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 60
            periodSeconds: 15
      volumes:
        - name: hadoop-datanode-storage
          persistentVolumeClaim:
            claimName: hadoop-datanode
        - name: hadoop-config-ro
          configMap:
            name: hadoop-config
            defaultMode: 0440
        - name: hadoop-config-rw
          emptyDir:
            medium: Memory
            sizeLimit: 20Mi
---
# ResourceManager Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resourcemanager
  namespace: data-platform
  labels:
    app: resourcemanager
    app.kubernetes.io/component: hadoop
    app.kubernetes.io/part-of: data-platform
spec:
  replicas: 1
  selector:
    matchLabels:
      app: resourcemanager
  template:
    metadata:
      labels:
        app: resourcemanager
        app.kubernetes.io/component: hadoop
        app.kubernetes.io/part-of: data-platform
    spec:
      serviceAccountName: hadoop-sa
      securityContext:
        fsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
      initContainers:
      - name: config-init
        image: airflow-spark:1.0.0
        imagePullPolicy: IfNotPresent
        securityContext:
          allowPrivilegeEscalation: false
          runAsUser: 1000
          runAsGroup: 1000
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: false
        command:
        - sh
        - -c
        - |
          set -ex
          echo "Copying configuration files..."
          mkdir -p /config-rw
          cp -r /config-mount/* /config-rw/
          chmod -R 755 /config-rw/
        resources:
          limits:
            cpu: "200m"
            memory: "256Mi"
          requests:
            cpu: "100m"
            memory: "128Mi"
        volumeMounts:
        - mountPath: /config-mount
          name: hadoop-config-ro
          readOnly: true
        - mountPath: /config-rw
          name: hadoop-config-rw
      containers:
        - name: resourcemanager
          image: airflow-spark:1.0.0
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            runAsUser: 1000
            runAsGroup: 1000
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          command: ["/bin/bash"]
          args:
            - "-c"
            - >
              export JAVA_TOOL_OPTIONS="--add-opens=java.base/java.lang=ALL-UNNAMED
              --add-opens=java.base/java.util=ALL-UNNAMED
              --add-opens=java.base/java.nio=ALL-UNNAMED
              --add-opens=java.base/java.lang.reflect=ALL-UNNAMED
              --add-opens=java.base/java.security=ALL-UNNAMED
              --add-opens=java.base/java.util.concurrent=ALL-UNNAMED
              --add-opens=java.base/java.net=ALL-UNNAMED" &&
              export YARN_RESOURCEMANAGER_OPTS="$JAVA_TOOL_OPTIONS" &&
              /bootstrap.sh resourcemanager
          ports:
            - containerPort: 8088
              name: web
            - containerPort: 8030
              name: scheduler
            - containerPort: 8031
              name: tracker
            - containerPort: 8032
              name: admin
          env:
            - name: SERVICE_TYPE
              value: hadoop
            - name: HADOOP_HOME
              value: /opt/hadoop
            - name: YARN_RESOURCEMANAGER_USER
              value: yarn
            - name: HADOOP_CONF_DIR
              value: /opt/hadoop/etc/hadoop
            - name: HADOOP_OPTS
              value: "--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.security=ALL-UNNAMED -Djava.security.properties=/opt/hadoop/etc/hadoop/java.security"
            - name: YARN_SECURE_LOG_DIR
              value: /opt/hadoop/logs
          envFrom:
            - configMapRef:
                name: hadoop-env
          resources:
            limits:
              cpu: "2"
              memory: "4Gi"
            requests:
              cpu: "1"
              memory: "2Gi"
          volumeMounts:
            - name: hadoop-config-rw
              mountPath: /opt/hadoop/etc/hadoop
            - name: hadoop-logs
              mountPath: /opt/hadoop/logs
          livenessProbe:
            httpGet:
              path: /ws/v1/cluster/info
              port: web
            initialDelaySeconds: 90
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /ws/v1/cluster/info
              port: web
            initialDelaySeconds: 60
            periodSeconds: 15
      volumes:
        - name: hadoop-logs
          persistentVolumeClaim:
            claimName: hadoop-logs
        - name: hadoop-config-ro
          configMap:
            name: hadoop-config
            defaultMode: 0440
        - name: hadoop-config-rw
          emptyDir:
            medium: Memory
            sizeLimit: 20Mi
---
# NodeManager Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodemanager
  namespace: data-platform
  labels:
    app: nodemanager
    app.kubernetes.io/component: hadoop
    app.kubernetes.io/part-of: data-platform
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nodemanager
  template:
    metadata:
      labels:
        app: nodemanager
        app.kubernetes.io/component: hadoop
        app.kubernetes.io/part-of: data-platform
    spec:
      serviceAccountName: hadoop-sa
      securityContext:
        fsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
      initContainers:
      - name: config-init
        image: airflow-spark:1.0.0
        imagePullPolicy: IfNotPresent
        securityContext:
          allowPrivilegeEscalation: false
          runAsUser: 1000
          runAsGroup: 1000
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: false
        command:
        - sh
        - -c
        - |
          set -ex
          echo "Copying configuration files..."
          mkdir -p /config-rw
          cp -r /config-mount/* /config-rw/
          chmod -R 755 /config-rw/
        resources:
          limits:
            cpu: "200m"
            memory: "256Mi"
          requests:
            cpu: "100m"
            memory: "128Mi"
        volumeMounts:
        - mountPath: /config-mount
          name: hadoop-config-ro
          readOnly: true
        - mountPath: /config-rw
          name: hadoop-config-rw
      containers:
        - name: nodemanager
          image: airflow-spark:1.0.0
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            runAsUser: 1000
            runAsGroup: 1000
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          command: ["/bin/bash"]
          args:
          - -c
          - |
            export JAVA_TOOL_OPTIONS="--add-opens=java.base/java.lang=ALL-UNNAMED \
            --add-opens=java.base/java.util=ALL-UNNAMED \
            --add-opens=java.base/java.nio=ALL-UNNAMED" &&
            /bootstrap.sh nodemanager
          ports:
            - containerPort: 8042
              name: web
          env:
            - name: SERVICE_TYPE
              value: hadoop
            - name: HADOOP_HOME
              value: /opt/hadoop
            - name: YARN_NODEMANAGER_USER
              value: yarn
            - name: HADOOP_CONF_DIR
              value: /opt/hadoop/etc/hadoop
            - name: HADOOP_OPTS
              value: "-Djava.security.properties=/opt/hadoop/etc/hadoop/java.security"
          envFrom:
            - configMapRef:
                name: hadoop-env
          resources:
            limits:
              cpu: "2"
              memory: "4Gi"
            requests:
              cpu: "1"
              memory: "2Gi"
          volumeMounts:
            - name: hadoop-config-rw
              mountPath: /opt/hadoop/etc/hadoop
            - name: hadoop-logs
              mountPath: /opt/hadoop/logs
          livenessProbe:
            httpGet:
              path: /node
              port: web
            initialDelaySeconds: 90
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /node
              port: web
            initialDelaySeconds: 60
            periodSeconds: 15
      volumes:
        - name: hadoop-logs
          persistentVolumeClaim:
            claimName: hadoop-logs
        - name: hadoop-config-ro
          configMap:
            name: hadoop-config
            defaultMode: 0440
        - name: hadoop-config-rw
          emptyDir:
            medium: Memory
            sizeLimit: 20Mi
---
# HistoryServer Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: historyserver
  namespace: data-platform
  labels:
    app: historyserver
    app.kubernetes.io/component: hadoop
    app.kubernetes.io/part-of: data-platform
spec:
  replicas: 1
  selector:
    matchLabels:
      app: historyserver
  template:
    metadata:
      labels:
        app: historyserver
        app.kubernetes.io/component: hadoop
        app.kubernetes.io/part-of: data-platform
    spec:
      serviceAccountName: hadoop-sa
      securityContext:
        fsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
      initContainers:
      - name: config-init
        image: airflow-spark:1.0.0
        imagePullPolicy: IfNotPresent
        securityContext:
          allowPrivilegeEscalation: false
          runAsUser: 1000
          runAsGroup: 1000
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: false
        command:
        - sh
        - -c
        - |
          set -ex
          echo "Copying configuration files..."
          mkdir -p /config-rw
          cp -r /config-mount/* /config-rw/
          chmod -R 755 /config-rw/
        resources:
          limits:
            cpu: "200m"
            memory: "256Mi"
          requests:
            cpu: "100m"
            memory: "128Mi"
        volumeMounts:
        - mountPath: /config-mount
          name: hadoop-config-ro
          readOnly: true
        - mountPath: /config-rw
          name: hadoop-config-rw
      containers:
        - name: historyserver
          image: airflow-spark:1.0.0
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            runAsUser: 1000
            runAsGroup: 1000
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          command: ["/bin/bash"]
          args:
          - -c
          - |
            export JAVA_TOOL_OPTIONS="--add-opens=java.base/java.lang=ALL-UNNAMED \
            --add-opens=java.base/java.util=ALL-UNNAMED \
            --add-opens=java.base/java.nio=ALL-UNNAMED" &&
            /bootstrap.sh historyserver
          ports:
            - containerPort: 8188
              name: web
            - containerPort: 8190
              name: https
          env:
            - name: SERVICE_TYPE
              value: hadoop
            - name: HADOOP_HOME
              value: /opt/hadoop
            - name: YARN_TIMELINE_SERVICE_HOSTNAME
              value: historyserver
            - name: HADOOP_CONF_DIR
              value: /opt/hadoop/etc/hadoop
            - name: HADOOP_OPTS
              value: "-Djava.security.properties=/opt/hadoop/etc/hadoop/java.security"
          envFrom:
            - configMapRef:
                name: hadoop-env
          resources:
            limits:
              cpu: "1"
              memory: "2Gi"
            requests:
              cpu: "500m"
              memory: "1Gi"
          volumeMounts:
            - name: hadoop-config-rw
              mountPath: /opt/hadoop/etc/hadoop
            - name: hadoop-historyserver
              mountPath: /opt/hadoop/logs/userlogs
          livenessProbe:
            httpGet:
              path: /ws/v1/timeline/
              port: web
            initialDelaySeconds: 90
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /ws/v1/timeline/
              port: web
            initialDelaySeconds: 60
            periodSeconds: 15
      volumes:
        - name: hadoop-historyserver
          persistentVolumeClaim:
            claimName: hadoop-historyserver
        - name: hadoop-config-ro
          configMap:
            name: hadoop-config
            defaultMode: 0440
        - name: hadoop-config-rw
          emptyDir:
            medium: Memory
            sizeLimit: 20Mi
