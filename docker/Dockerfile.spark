FROM airflow-hadoop:latest

USER root

# Create airflow user and group with specific UID/GID
# Note: The airflow user is already in the base image, but we ensure consistent UID/GID
RUN groupadd -g 50000 airflow 2>/dev/null || groupmod -g 50000 airflow && \
    useradd -u 50000 -g airflow -d /home/airflow -m -s /bin/bash airflow 2>/dev/null || usermod -u 50000 -g airflow airflow

# Install only necessary dependencies that aren't in previous images
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        net-tools \
        scala && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Setup Spark
ENV SPARK_VERSION=3.4.4
ENV HADOOP_VERSION=3

# Download and setup Spark
COPY downloads/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz /tmp/
RUN tar -xzf /tmp/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /opt/ && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /opt/spark && \
    rm /tmp/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Set Spark environment variables
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.5-src.zip

# Add JDK 17 compatibility flags for Spark
ENV SPARK_JAVA_OPTS="--add-opens=java.base/java.lang=ALL-UNNAMED \
    --add-opens=java.base/java.util=ALL-UNNAMED \
    --add-opens=java.base/java.io=ALL-UNNAMED \
    --add-opens=java.base/java.nio=ALL-UNNAMED \
    --add-opens=java.base/java.net=ALL-UNNAMED \
    --add-opens=java.base/java.util.concurrent=ALL-UNNAMED \
    --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED \
    --add-opens=java.base/java.math=ALL-UNNAMED \
    --add-exports=java.base/sun.nio.ch=ALL-UNNAMED \
    --add-exports=java.base/sun.security.action=ALL-UNNAMED"

# Copy configurations first
COPY spark/spark-defaults.conf /opt/spark/conf/
COPY spark/spark-env.sh /opt/spark/conf/
COPY spark/entrypoint.sh /spark-entrypoint.sh
COPY spark/combined-entrypoint.sh /combined-entrypoint.sh

# Add JDK 17 flags to spark-env.sh
RUN echo "export SPARK_JAVA_OPTS=\"${SPARK_JAVA_OPTS}\"" >> /opt/spark/conf/spark-env.sh

# Create necessary directories and set appropriate permissions
# Combining multiple RUN commands to reduce layers
RUN mkdir -p /opt/spark/logs /opt/spark/work && \
    chmod +x /spark-entrypoint.sh /combined-entrypoint.sh && \
    chmod 644 /opt/spark/conf/spark-defaults.conf && \
    chmod 755 /opt/spark/conf/spark-env.sh && \
    # Update Hadoop env
    echo "export SPARK_HOME=/opt/spark" >> ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh && \
    echo "export PATH=\$PATH:\$SPARK_HOME/bin:\$SPARK_HOME/sbin" >> ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh && \
    # Set proper ownership for all directories
    chown -R airflow:airflow /opt/spark && \
    chown -R airflow:airflow ${HADOOP_HOME} && \
    # No need to create .ssh directories as they're handled in the hadoop image
    # Set appropriate permissions (more restrictive)
    chmod -R 750 /opt/spark /opt/hadoop

EXPOSE 4040 7077 8080 8081

WORKDIR /opt/airflow

USER airflow

RUN pip install --no-cache-dir \
    pyspark==${SPARK_VERSION} \
    apache-airflow-providers-apache-spark \
    apache-airflow-providers-openlineage>=2.1.0 --upgrade

ENTRYPOINT ["/combined-entrypoint.sh"]
