version: '3.8'  # Upgrade to latest compatible version

services:
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      # Use secrets for passwords instead of environment variables
      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password
      - POSTGRES_DB=airflow
    ports:
      # Bind to localhost for security
      - "127.0.0.1:5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - hadoop_network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
    # Add security options
    security_opt:
      - no-new-privileges:true
    # Add read-only filesystem where possible
    read_only: false  # Can't be true for postgres
    # Add tmpfs for temp files
    tmpfs:
      - /tmp
    secrets:
      - postgres_password

  airflow-webserver:
    image: airflow-spark:latest
    build:
      context: .
      dockerfile: docker/Dockerfile.spark
    ports:
      # Bind to localhost for security
      - "127.0.0.1:8080:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      # Use secrets for connection strings
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN_CMD=cat /run/secrets/airflow_db_conn
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
      - SPARK_HOME=/opt/spark
      # Improve security settings
      - AIRFLOW__WEBSERVER__WEB_SERVER_SSL_CERT=/run/secrets/airflow_cert
      - AIRFLOW__WEBSERVER__WEB_SERVER_SSL_KEY=/run/secrets/airflow_key
      - AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX=True
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=False
      - AIRFLOW__CORE__SECURE_MODE=True
      - AIRFLOW__CORE__DAG_DISCOVERY_SAFE_MODE=True
      - AIRFLOW__CORE__EXECUTE_TASKS_NEW_PYTHON_INTERPRETER=True
      - AIRFLOW__WEBSERVER__RBAC=True
      # Set non-root user
      - RUN_AS_USER=airflow
    volumes:
      - ./dags:/opt/airflow/dags:ro  # Read-only for security
      - ./spark/tests:/opt/spark/tests:ro  # Read-only for security
      - airflow_logs:/opt/airflow/logs
    networks:
      - hadoop_network
    depends_on:
      postgres:
        condition: service_healthy
    entrypoint: >
      /bin/bash -c "
      airflow db init &&
      airflow users create -r Admin -u admin -p $(cat /run/secrets/airflow_admin_password) -e admin@example.com -f Admin -l User &&
      airflow webserver
      "
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    # Add security options
    security_opt:
      - no-new-privileges:true
    # Add read-only filesystem where possible
    read_only: false  # Can't be fully read-only due to runtime requirements
    # Add tmpfs for temp files
    tmpfs:
      - /tmp
    secrets:
      - airflow_db_conn
      - airflow_admin_password
      - airflow_cert
      - airflow_key
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G

  airflow-scheduler:
    image: airflow-spark:latest
    build:
      context: .
      dockerfile: docker/Dockerfile.spark
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      # Use secrets for connection strings
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN_CMD=cat /run/secrets/airflow_db_conn
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
      - SPARK_HOME=/opt/spark
      # Improve security settings
      - AIRFLOW__CORE__SECURE_MODE=True
      - AIRFLOW__CORE__DAG_DISCOVERY_SAFE_MODE=True
      - AIRFLOW__CORE__EXECUTE_TASKS_NEW_PYTHON_INTERPRETER=True
      # Set non-root user
      - RUN_AS_USER=airflow
    volumes:
      - ./dags:/opt/airflow/dags:ro  # Read-only for security
      - ./spark/tests:/opt/spark/tests:ro  # Read-only for security
      - airflow_logs:/opt/airflow/logs
    networks:
      - hadoop_network
    depends_on:
      postgres:
        condition: service_healthy
    entrypoint: >
      /bin/bash -c "
      airflow db upgrade &&
      airflow scheduler
      "
    # Add security options
    security_opt:
      - no-new-privileges:true
    # Add read-only filesystem where possible
    read_only: false  # Can't be fully read-only due to runtime requirements
    # Add tmpfs for temp files
    tmpfs:
      - /tmp
    secrets:
      - airflow_db_conn
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G

  namenode:
    image: airflow-spark:latest
    build:
      context: .
      dockerfile: docker/Dockerfile.spark
    hostname: namenode
    # Use a non-root user where possible and implement user namespaces
    user: root  # Hadoop often needs root for now, but consider using user namespaces
    env_file:
      - ./hadoop.env
    ports:
      # Bind to localhost for security
      - "127.0.0.1:9870:9870"  # Hadoop namenode web UI
      - "127.0.0.1:9000:9000"  # HDFS
    volumes:
      - hadoop_namenode:/opt/hadoop/dfs/name
      - hadoop_logs:/opt/hadoop/logs
    networks:
      - hadoop_network  
    environment:
      - SERVICE_TYPE=hadoop
      - HADOOP_HOME=/opt/hadoop
      - HDFS_NAMENODE_USER=hadoop
      - HDFS_DATANODE_USER=hadoop
      - HDFS_SECONDARYNAMENODE_USER=hadoop
      - YARN_RESOURCEMANAGER_USER=hadoop
      - YARN_NODEMANAGER_USER=hadoop
      # Add security environment variables
      - HADOOP_SECURE_MODE=true
      - HADOOP_OPTS=-Djava.security.properties=/opt/hadoop/conf/java.security
    command: ["namenode"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3
    # Add security options
    security_opt:
      - no-new-privileges:true
    # Add tmpfs for temp files
    tmpfs:
      - /tmp
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
    configs:
      - source: hadoop_java_security
        target: /opt/hadoop/conf/java.security

  datanode:
    image: airflow-spark:latest
    build:
      context: .
      dockerfile: docker/Dockerfile.spark
    hostname: datanode
    # Use a non-root user where possible and implement user namespaces
    user: root  # Hadoop often needs root for now, but consider using user namespaces
    env_file:
      - ./hadoop.env
    networks:
      - hadoop_network
    volumes:
      - hadoop_datanode:/opt/hadoop/dfs/data
    environment:
      - SERVICE_TYPE=hadoop
      - HADOOP_HOME=/opt/hadoop
      - HDFS_NAMENODE_USER=hadoop
      - HDFS_DATANODE_USER=hadoop
      # Add security environment variables
      - HADOOP_SECURE_MODE=true
      - HADOOP_OPTS=-Djava.security.properties=/opt/hadoop/conf/java.security
    command: ["datanode"]
    depends_on:
      - namenode
    # Add security options
    security_opt:
      - no-new-privileges:true
    # Add tmpfs for temp files
    tmpfs:
      - /tmp
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
    configs:
      - source: hadoop_java_security
        target: /opt/hadoop/conf/java.security

  resourcemanager:
    image: airflow-spark:latest
    build:
      context: .
      dockerfile: docker/Dockerfile.spark
    hostname: resourcemanager
    # Use a non-root user where possible and implement user namespaces
    user: root  # Hadoop often needs root for now, but consider using user namespaces
    env_file:
      - ./hadoop.env
    ports:
      # Bind to localhost for security
      - "127.0.0.1:8088:8088"  # YARN web UI
    command: ["resourcemanager"]
    depends_on:
      - namenode
    networks:
      - hadoop_network
    environment:
      - SERVICE_TYPE=hadoop
      - HADOOP_HOME=/opt/hadoop
      - YARN_RESOURCEMANAGER_USER=hadoop
      - YARN_NODEMANAGER_USER=hadoop
      # Add security environment variables
      - HADOOP_SECURE_MODE=true
      - HADOOP_OPTS=-Djava.security.properties=/opt/hadoop/conf/java.security
    # Add security options
    security_opt:
      - no-new-privileges:true
    # Add tmpfs for temp files
    tmpfs:
      - /tmp
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
    configs:
      - source: hadoop_java_security
        target: /opt/hadoop/conf/java.security

  nodemanager:
    image: airflow-spark:latest
    build:
      context: .
      dockerfile: docker/Dockerfile.spark
    hostname: nodemanager
    # Use a non-root user where possible and implement user namespaces
    user: root  # Hadoop often needs root for now, but consider using user namespaces
    env_file:
      - ./hadoop.env
    command: ["nodemanager"]
    depends_on:
      - namenode
      - resourcemanager
    networks:
      - hadoop_network
    environment:
      - SERVICE_TYPE=hadoop
      - HADOOP_HOME=/opt/hadoop
      - YARN_RESOURCEMANAGER_USER=hadoop
      - YARN_NODEMANAGER_USER=hadoop
      # Add security environment variables
      - HADOOP_SECURE_MODE=true
      - HADOOP_OPTS=-Djava.security.properties=/opt/hadoop/conf/java.security
    # Add security options
    security_opt:
      - no-new-privileges:true
    # Add tmpfs for temp files
    tmpfs:
      - /tmp
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
    configs:
      - source: hadoop_java_security
        target: /opt/hadoop/conf/java.security

  historyserver:
    image: airflow-spark:latest
    hostname: historyserver
    # Use a non-root user where possible and implement user namespaces
    user: root  # Hadoop often needs root for now, but consider using user namespaces
    env_file:
      - ./hadoop.env
    ports:
      # Bind to localhost for security
      - "127.0.0.1:8188:8188"
    depends_on:
      - namenode
      - resourcemanager
    networks:
      - hadoop_network
    environment:
      - SERVICE_TYPE=hadoop
      - HADOOP_HOME=/opt/hadoop
      - YARN_TIMELINE_SERVICE_HOSTNAME=historyserver
      # Add security environment variables
      - HADOOP_SECURE_MODE=true
      - HADOOP_OPTS=-Djava.security.properties=/opt/hadoop/conf/java.security
    volumes:
      - hadoop_historyserver:/opt/hadoop/logs/userlogs
    command: ["historyserver"]
    # Add security options
    security_opt:
      - no-new-privileges:true
    # Add tmpfs for temp files
    tmpfs:
      - /tmp
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
    configs:
      - source: hadoop_java_security
        target: /opt/hadoop/conf/java.security
  
  spark-master:
    image: airflow-spark:latest
    hostname: spark-master
    ports:
      # Bind to localhost for security
      - "127.0.0.1:8181:8080"
      - "127.0.0.1:7077:7077"
      - "127.0.0.1:4040-4045:4040-4045"
    configs:
      - source: spark_defaults
        target: /opt/spark/conf/spark-defaults.conf
      - source: spark_env
        target: /opt/spark/conf/spark-env.sh
      - source: spark_policy
        target: /opt/spark/conf/spark.policy
    environment:
      - SPARK_MODE=master
      - SPARK_LOG_DIR=/opt/spark/logs
      # Add security environment variables
      - SPARK_AUTH_SECRET_FILE=/run/secrets/spark_secret
      # Set non-root user
      - RUN_AS_USER=hadoop
    networks:
      - hadoop_network
    volumes:
      - spark_logs:/opt/spark/logs
      - ./spark/tests:/opt/spark/tests:ro  # Read-only for security
      - ./spark/fairscheduler.xml:/opt/spark/conf/fairscheduler.xml:ro  # Read-only for security
    user: hadoop
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 7077 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    command: |
      bash -c "
      rm -f /opt/spark/logs/* &&
      /opt/spark/sbin/start-master.sh &&
      tail -f /opt/spark/logs/*"
    # Add security options
    security_opt:
      - no-new-privileges:true
    # Add read-only filesystem where possible
    read_only: false  # Can't be fully read-only due to runtime requirements
    # Add tmpfs for temp files
    tmpfs:
      - /tmp
    secrets:
      - spark_secret
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G

  spark-worker:
    image: airflow-spark:latest
    depends_on:
      spark-master:
        condition: service_healthy
    hostname: spark-worker
    ports:
      # Bind to localhost for security
      - "127.0.0.1:8183:8083"  # Map worker UI port to host
      - "127.0.0.1:8082:8082"
    configs:
      - source: spark_defaults
        target: /opt/spark/conf/spark-defaults.conf
      - source: spark_env
        target: /opt/spark/conf/spark-env.sh
      - source: spark_policy
        target: /opt/spark/conf/spark.policy
    environment:
      - SPARK_WORKER_CORES=2 
      - SPARK_WORKER_MEMORY=4g 
      - SPARK_LOCAL_HOSTNAME=spark-worker
      - SPARK_LOG_DIR=/opt/spark/logs
      # Add security environment variables
      - SPARK_AUTH_SECRET_FILE=/run/secrets/spark_secret
      # Set non-root user
      - RUN_AS_USER=hadoop
    volumes:
      - spark_logs:/opt/spark/logs
      - spark_work_dir:/opt/spark/work-dir
      - ./spark/tests:/opt/spark/tests:ro  # Read-only for security
      - ./spark/fairscheduler.xml:/opt/spark/conf/fairscheduler.xml:ro  # Read-only for security
    networks:
      - hadoop_network
    user: hadoop
    command: |
      bash -c '
      mkdir -p /opt/spark/work-dir && \
      /opt/spark/sbin/start-worker.sh spark://spark-master:7077 \
        --webui-port 8083 \
        --port 8082 \
        --host spark-worker && \
      tail -f /opt/spark/logs/*'
    # Add security options
    security_opt:
      - no-new-privileges:true
    # Add read-only filesystem where possible
    read_only: false  # Can't be fully read-only due to runtime requirements
    # Add tmpfs for temp files
    tmpfs:
      - /tmp
    secrets:
      - spark_secret
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G

configs:
  spark_defaults:
    file: ./spark/spark-defaults.conf
  spark_env:
    file: ./spark/spark-env.sh
  spark_policy:
    file: ./spark/spark.policy
  hadoop_java_security:
    file: ./hadoop/java.security

secrets:
  postgres_password:
    file: ./secrets/postgres_password.txt
  airflow_db_conn:
    file: ./secrets/airflow_db_conn.txt
  airflow_admin_password:
    file: ./secrets/airflow_admin_password.txt
  airflow_cert:
    file: ./secrets/airflow.crt
  airflow_key:
    file: ./secrets/airflow.key
  spark_secret:
    file: ./secrets/spark_secret.txt

volumes:
  postgres_data:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_logs:
  hadoop_historyserver:
  spark_work_dir:
  spark_logs:
  airflow_logs:

networks:
  hadoop_network:
    driver: bridge
    # Add network isolation
    ipam:
      config:
        - subnet: 172.28.0.0/16
          gateway: 172.28.0.1
    # Optional: Enable encryption for network traffic
    # driver_opts:
    #   encrypted: "true"
